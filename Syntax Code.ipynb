{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl3PWtuNx38d"
      },
      "source": [
        "# **DATA PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME850rNLlhqq"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4AwCyGtl2QB",
        "outputId": "108e56e0-400b-41d6-a934-ebdd9cd640d4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Fd0kPHvCl_EV",
        "outputId": "e92e965f-6c45-4097-a1c9-bcab7cd4e017"
      },
      "outputs": [],
      "source": [
        "# Membaca data dari file CSV\n",
        "data = pd.read_excel('/content/drive/MyDrive/Skripsi/data_skripsi/data_label_0_1.xlsx')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZNzbJU1mGnG",
        "outputId": "1cf63a7e-c46c-4832-9ff5-c8c398d9ff32"
      },
      "outputs": [],
      "source": [
        "#Case Folding\n",
        "data['textDisplay'] = data['textDisplay'].str.lower()\n",
        "print('Case Folding Result : \\n')\n",
        "print(data['textDisplay'].head(5))\n",
        "print('\\n\\n\\n')\n",
        "import string, re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CyV87ckH0can",
        "outputId": "d7a80e62-cfea-452c-c5cc-96f4d8b2093a"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "HS7NTZlImP6C",
        "outputId": "1b201d83-a7ea-4ac9-c719-1ff61b669ec1"
      },
      "outputs": [],
      "source": [
        "#Tokenizing\n",
        "def remove_comments_special(text):\n",
        "    #menghapus karakter khusus dalam teks, seperti tab, newline, dan backslash\n",
        "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\" \").replace('.',\" \").replace(',',\" \")\n",
        "    # remove non ASCII (emoticon, chinese word, .etc)\n",
        "    text = text.encode('ascii', 'replace').decode('ascii')\n",
        "    # remove mention, link, hashtag\n",
        "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
        "    # remove incomplete URL\n",
        "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
        "data['textDisplay'] = data['textDisplay'].apply(remove_comments_special)\n",
        "\n",
        "#menghapus angka dalam teks dengan menggunakan regular expression\n",
        "def remove_number(text):\n",
        "    return  re.sub(r\"\\d+\", \" \", text)\n",
        "data['textDisplay'] = data['textDisplay'].apply(remove_number)\n",
        "#menghapus emotikon dari teks menggunakan regular expression\n",
        "def remove_emoticons(text):\n",
        "    emoticon_pattern = re.compile(\"[\"\n",
        "                                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                                 u\"\\U0001F300-\\U0001F5FF\"  # simbol & piktogram\n",
        "                                 u\"\\U0001F680-\\U0001F6FF\"  # transportasi & simbol peralatan\n",
        "                                 u\"\\U0001F1E0-\\U0001F1FF\"  # bendera negara\n",
        "                                 u\"\\U00002702-\\U000027B0\"  # simbol lainnya\n",
        "                                 u\"\\U000024C2-\\U0001F251\"\n",
        "                                 \"]+\", flags=re.UNICODE)\n",
        "    return emoticon_pattern.sub(r'', text)\n",
        "data['textDisplay'] = data['textDisplay'].apply(remove_emoticons)\n",
        "#data['review'] = data['review'].apply(remove_number)\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    return text.translate(translator).replace(\",\", \" \")\n",
        "data['textDisplay'] = data['textDisplay'].apply(remove_punctuation)\n",
        "# menghapus spasi yang ada di awal dan akhir teks menggunakan str.strip()\n",
        "def remove_whitespace_LT(text):\n",
        "    return text.strip()\n",
        "data['textDisplay'] = data['textDisplay'].apply(remove_whitespace_LT)\n",
        "#menghapus spasi berlebih dalam teks dengan menggantinya dengan satu spasi menggunakan regular expression\n",
        "def remove_extra_spaces(text):\n",
        "    return re.sub(r'\\s+', ' ', text)\n",
        "data['textDisplay'] = data['textDisplay'].apply(remove_extra_spaces)\n",
        "#mengganti multiple whitespace (spasi berturut-turut) dengan satu spasi menggunakan regular expression\n",
        "def remove_whitespace_multiple(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "data['textDisplay'] = data['textDisplay'].apply(remove_whitespace_multiple)\n",
        "#menghapus huruf yang berulang dalam teks\n",
        "def remove_repeated_letters(text):\n",
        "    return re.sub(r\"(.)\\1+\", r\"\\1\", text)\n",
        "data['textDisplay'] = data['textDisplay'].apply(remove_repeated_letters)\n",
        "\n",
        "#menghapus kata-kata yang terdiri dari satu huruf (single character) dalam teks menggunakan regular expression\n",
        "def filter_length(text):\n",
        "    text_filtered = ' '.join(word for word in re.split(r'\\s', text) if 1 < len(word))\n",
        "    return text_filtered\n",
        "data['textDisplay'] = data['textDisplay'].apply(filter_length)\n",
        "\n",
        "\n",
        "\n",
        "#mengubah kalimat menjadi token\n",
        "def word_tokenize_wrapper(text):\n",
        "    return word_tokenize(text)\n",
        "data['comments_tokens'] = data['textDisplay'].apply(word_tokenize_wrapper)\n",
        "print('Tokenizing Result : \\n')\n",
        "print(data['comments_tokens'].head())\n",
        "print('\\n\\n\\n')\n",
        "\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsoUYVg-mUQQ"
      },
      "outputs": [],
      "source": [
        "#Spelling Normalization\n",
        "normalizad_word = pd.read_csv(\"https://raw.githubusercontent.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset/master/kamus_singkatan.csv\", sep=\";\", header=None)\n",
        "normalizad_word_dict = {}\n",
        "\n",
        "for index, row in normalizad_word.iterrows():\n",
        "    if row[0] not in normalizad_word_dict:\n",
        "        normalizad_word_dict[row[0]] = row[1]\n",
        "\n",
        "def normalized_term(document):\n",
        "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
        "data['comments_normalized'] = data['comments_tokens'].apply(normalized_term)\n",
        "\n",
        "normalizad_word2 = pd.read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRQS3tlUL5EcxYqbbYzFLHmHaqm2npjY-DLyz0dzwMIcUVhfoVWKuhR52P9YCqbAyY9zCgT66JVutWA/pub?output=csv\",header=None)\n",
        "normalizad_word_dict2 = {}\n",
        "\n",
        "for index, row in normalizad_word2.iterrows():\n",
        "    if row[0] not in normalizad_word_dict2:\n",
        "        normalizad_word_dict2[row[0]] = row[1]\n",
        "def normalized_term2(document):\n",
        "    return [normalizad_word_dict2[term] if term in normalizad_word_dict2 else term for term in document]\n",
        "data['comments_normalized'] = data['comments_normalized'].apply(normalized_term2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SREU6y3bZJ5q",
        "outputId": "3d88995d-15fe-498b-c3d6-8705107621ca"
      },
      "outputs": [],
      "source": [
        "#menyatukan kembali tokenizer menjadi kalimat\n",
        "def detokenize(token_list):\n",
        "    # Menggabungkan token dengan spasi sebagai pemisah\n",
        "    text = ' '.join(token_list)\n",
        "    return text\n",
        "\n",
        "data['comments_join'] = data['comments_normalized'].apply(detokenize)\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJmUvxMDaWdd",
        "outputId": "966212d2-be22-4977-fe42-7509334a18bb"
      },
      "outputs": [],
      "source": [
        "# menghapus spasi yang ada di awal dan akhir teks menggunakan str.strip()\n",
        "def remove_whitespace_LT(text):\n",
        "    return text.strip()\n",
        "data['comments_join'] = data['comments_join'].apply(remove_whitespace_LT)\n",
        "#menghapus spasi berlebih dalam teks dengan menggantinya dengan satu spasi menggunakan regular expression\n",
        "def remove_extra_spaces(text):\n",
        "    return re.sub(r'\\s+', ' ', text)\n",
        "data['comments_join'] = data['comments_join'].apply(remove_extra_spaces)\n",
        "#mengganti multiple whitespace (spasi berturut-turut) dengan satu spasi menggunakan regular expression\n",
        "def remove_whitespace_multiple(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "data['comments_join'] = data['comments_join'].apply(remove_whitespace_multiple)\n",
        "\n",
        "#mengubah kalimat menjadi token\n",
        "def word_tokenize_wrapper(text):\n",
        "    return word_tokenize(text)\n",
        "data['comments_tokens_final'] = data['comments_join'].apply(word_tokenize_wrapper)\n",
        "print('Tokenizing Result : \\n')\n",
        "print(data['comments_tokens_final'].head())\n",
        "print('\\n\\n\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qqvck_hVmYPm",
        "outputId": "38b1063c-7924-40ea-8e13-4beb23672811"
      },
      "outputs": [],
      "source": [
        "#Stopword Removal\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Skripsi/data_skripsi/stop word indo (3).txt'  # Ganti dengan jalur file yang sesuai\n",
        "\n",
        "# Baca isi file teks ke dalam list\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Mengubah setiap baris menjadi string dengan kutipan ganda\n",
        "formatted_lines = ['\"' + line.strip() + '\"' for line in lines]\n",
        "\n",
        "# Menggabungkan baris-baris menjadi satu string dengan koma sebagai pemisah\n",
        "formatted_text = \", \".join(formatted_lines)\n",
        "\n",
        "print(formatted_text)\n",
        "\n",
        "# List kata stopwords\n",
        "stopwords_list = formatted_text\n",
        "def stopwords_removal(words):\n",
        "    return [word for word in words if word not in stopwords_list]\n",
        "\n",
        "\n",
        "\n",
        "data['comments_tokens_sw'] = data['comments_tokens_final'].apply(stopwords_removal)\n",
        "data.head(100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "LRzLzMgRbJCi",
        "outputId": "2685177b-6d0e-4bb3-fd04-adcfdfa31e7e"
      },
      "outputs": [],
      "source": [
        "print(data['comments_tokens_sw'].head())\n",
        "\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DK-ip7ImbMz",
        "outputId": "176a059e-e6aa-4a02-86d0-e9a3adbfe240"
      },
      "outputs": [],
      "source": [
        "#Stemming\n",
        "# import Sastrawi package\n",
        "! pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "# create stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "#stemmed\n",
        "def stemmed_wrapper(term):\n",
        "    return stemmer.stem(term)\n",
        "term_dict = {}\n",
        "for document in data['comments_tokens_sw']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = ' '\n",
        "print(len(term_dict))\n",
        "print(\"------------------------\")\n",
        "for term in term_dict:\n",
        "    term_dict[term] = stemmed_wrapper(term)\n",
        "    print(term,\":\" ,term_dict[term])\n",
        "print(term_dict)\n",
        "print(\"------------------------\")\n",
        "# apply stemmed term to dataframe\n",
        "def get_stemmed_term(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "data['comments_tokens_stemmed'] = data['comments_tokens_sw'].apply(get_stemmed_term)\n",
        "\n",
        "# data[\"Ulasan_clean\"] = [' '.join(map(str, l)) for l in data['comments_tokens_stemmed']]\n",
        "# data.to_excel(\"Data_Skripsi_Bersih_.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x5oj4nlrEBIG",
        "outputId": "43987372-f953-483e-e796-388cc66f689b"
      },
      "outputs": [],
      "source": [
        "#Stopword Removal\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Skripsi/data_skripsi/stop word indo (3).txt'  # Ganti dengan jalur file yang sesuai\n",
        "\n",
        "# Baca isi file teks ke dalam list\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Mengubah setiap baris menjadi string dengan kutipan ganda\n",
        "formatted_lines = ['\"' + line.strip() + '\"' for line in lines]\n",
        "\n",
        "# Menggabungkan baris-baris menjadi satu string dengan koma sebagai pemisah\n",
        "formatted_text = \", \".join(formatted_lines)\n",
        "\n",
        "print(formatted_text)\n",
        "\n",
        "# List kata stopwords\n",
        "stopwords_list = formatted_text\n",
        "def stopwords_removal(words):\n",
        "    return [word for word in words if word not in stopwords_list]\n",
        "\n",
        "\n",
        "\n",
        "data['comments_tokens_stemmed'] = data['comments_tokens_stemmed'].apply(stopwords_removal)\n",
        "data.head(100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkzYkX3UE71r"
      },
      "outputs": [],
      "source": [
        "data[\"Ulasan_clean\"] = [' '.join(map(str, l)) for l in data['comments_tokens_stemmed']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MywYJt5MIFxf"
      },
      "outputs": [],
      "source": [
        "def filter_length(text):\n",
        "    text_filtered = ' '.join(word for word in re.split(r'\\s', text) if len(word) >= 4)\n",
        "    return text_filtered\n",
        "\n",
        "data['Ulasan_clean_fix'] = data['Ulasan_clean'].apply(filter_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "zk9wome1EbNW",
        "outputId": "0bca8854-f77d-4bc3-f37e-76192d9a292a"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrU0zfNeKHUc"
      },
      "outputs": [],
      "source": [
        "data.to_excel(\"Data_Skripsi_Bersih_Fix_.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "sQLu_MXym_4O",
        "outputId": "dcf208ea-b618-45c5-f1b1-66d7b39b528f"
      },
      "outputs": [],
      "source": [
        "###WordCloud\n",
        "#Import Library untuk WordCloud\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "text = \" \".join(title for title in data[\"Ulasan_clean_fix\"])\n",
        "word_cloud = WordCloud(collocations = False, background_color = 'white',\n",
        "                        width = 2048, height = 1080).generate(text)\n",
        "#Menyimpan Gambar WordCloud\n",
        "word_cloud.to_file('wordcloud.png')\n",
        "#Menampilkan Hasil WordCloud\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "nwm7SyaPnEp3",
        "outputId": "041cce52-3e43-41bd-cc35-f6cb041188b7"
      },
      "outputs": [],
      "source": [
        "#load data yang sudah bersih\n",
        "df_preprocessed=pd.read_excel(\"/content/Data_Skripsi_Bersih_Fix_ .xlsx\")\n",
        "df_preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwKuYv1oi1yC"
      },
      "outputs": [],
      "source": [
        "# Menghapus baris yang tidak memiliki kata dalam kolom Ulasan_Clean dan memiliki nilai NaN\n",
        "df_preprocessed = df_preprocessed.dropna(subset=['Ulasan_clean_fix'])\n",
        "\n",
        "# Sekarang, Anda dapat melanjutkan dengan mengaplikasikan filter atau melakukan operasi lain pada DataFrame df_preprocessed\n",
        "\n",
        "df_preprocessed.to_excel(\"df_preprocessed.xlsx\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cIG0fRpl4q9",
        "outputId": "bf75d8d4-c92c-4abf-f55f-e6315f12af24"
      },
      "outputs": [],
      "source": [
        "print(df_preprocessed.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dc0BVICnHqF",
        "outputId": "64076884-4819-41df-9b96-f671232d39fc"
      },
      "outputs": [],
      "source": [
        "df_preprocessed.Ulasan_clean=df_preprocessed.Ulasan_clean.astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf-6LzHSyCME"
      },
      "source": [
        "**Word Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M07qbhjnnJ-O",
        "outputId": "78c3250b-98de-4415-ac0b-3b1655c422ff"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "from urllib.request import urlopen\n",
        "file = gzip.open(urlopen('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.vec.gz'))\n",
        "\n",
        "vocab_and_vectors = {}\n",
        "# put words as dict indexes and vectors as words values\n",
        "for line in file:\n",
        "    values = line.split()\n",
        "    word = values [0].decode('utf-8')\n",
        "    vector = np.asarray(values[1:], dtype='float32')\n",
        "    vocab_and_vectors[word] = vector\n",
        "\n",
        "##pemberian indeks kata\n",
        "df_preprocessed.Ulasan_clean=df_preprocessed.Ulasan_clean.astype(str)\n",
        "# more imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# how many features should the tokenizer extract\n",
        "features = 500\n",
        "tokenizer = Tokenizer(num_words = features)\n",
        "# fit the tokenizer on our text\n",
        "texts = df_preprocessed.Ulasan_clean\n",
        "tokenizer.fit_on_texts(texts)\n",
        "# get all words that the tokenizer knows\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "# put the tokens in a matrix\n",
        "X = tokenizer.texts_to_sequences(df_preprocessed[\"Ulasan_clean_fix\"].tolist())\n",
        "X = pad_sequences(X)\n",
        "# prepare the labels\n",
        "Y = df_preprocessed[\"label\"]\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "print(texts)\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = vocab_and_vectors.get(word)\n",
        "    # words that cannot be found will be set to 0\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Print sample of tokenized text\n",
        "sample_tokenized = texts.sample(5)\n",
        "for i, tokenized in sample_tokenized.items():\n",
        "  print(tokenized)\n",
        "  print(X[i])\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a5jnlUxENX3",
        "outputId": "eb8e084c-4d75-4978-afa3-ee306298bb72"
      },
      "outputs": [],
      "source": [
        "# Print sample of word embeddings\n",
        "sample_words = list(word_index.keys())[:5]  # Ambil beberapa kata dari indeks\n",
        "for word in sample_words:\n",
        "    word_index_value = word_index.get(word)\n",
        "    if word_index_value is not None:\n",
        "        embedding_vector = embedding_matrix[word_index_value]\n",
        "        print(f\"Word: {word}, Embedding Vector: {embedding_vector}\")\n",
        "    else:\n",
        "        print(f\"Word '{word}' not found in the vocabulary.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRpb7MFHyJr_"
      },
      "source": [
        "**Split Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yk2GXw9PnMvM",
        "outputId": "ccfacb65-ae5a-4ad9-bb3c-b8535fe28bfc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Pembagian Data\n",
        "# Pembagian train (pelatihan) dan testing\n",
        "Y = df_preprocessed[\"label\"]\n",
        "Y\n",
        "print(Y.shape)\n",
        "print(X.shape)\n",
        "\n",
        "# One hot encoding label\n",
        "import seaborn as sns\n",
        "Y = to_categorical(Y, num_classes= 2)\n",
        "Y\n",
        "# split in train and test 80:20\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.2, stratify=Y, random_state=14)\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)\n",
        "# grafik data training dan testing\n",
        "sns.countplot(Y_train,label='count')\n",
        "plt.ylabel('Frekuensi', fontsize=12)\n",
        "plt.xlabel('Label', fontsize=12)\n",
        "plt.show()\n",
        "sns.countplot(Y_test,label='count')\n",
        "plt.ylabel('Frekuensi', fontsize=12)\n",
        "plt.xlabel('Label', fontsize=12)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEOBjeGtyPJr"
      },
      "source": [
        "# **HYPERPARAMETER TUNING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sQPglNtq1nn"
      },
      "source": [
        "# **LEARNING RATE **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsaBUOKNp4jE",
        "outputId": "779dc715-ff0b-47be-9ae3-5898240c47a2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "# Hyperparameters range\n",
        "learning_rate_range = [0.0001, 0.001, 0.01, 0.1]\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(patience=5, monitor='val_accuracy', mode='max', verbose=1, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model_1.h5', save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
        "\n",
        "# Callbacks list\n",
        "callbacks_list = [early_stopping, model_checkpoint]\n",
        "\n",
        "#Build Model\n",
        "def create_model(learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False))\n",
        "    model.add(Bidirectional(GRU(16)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Tentukan jumlah percobaan yang ingin Anda lakukan\n",
        "num_trials = 1\n",
        "\n",
        "# Loop melalui setiap learning rate\n",
        "for lr in learning_rate_range:\n",
        "    for _ in range(num_trials):\n",
        "        # Buat model baru untuk setiap iterasi\n",
        "        model = create_model(lr)\n",
        "\n",
        "        # Pelatihan model\n",
        "        HistoryBiGRU = model.fit(X_train, Y_train, batch_size=64, epochs=100, validation_data=(X_test, Y_test), callbacks=callbacks_list)\n",
        "\n",
        "        val_accuracy = HistoryBiGRU.history['val_accuracy'][-1]\n",
        "\n",
        "        print(f\"Trial {_ + 1} Hyperparameters:\")\n",
        "        print(f\"Learning Rate: {lr}\")\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq1X_7hgy0Wt"
      },
      "source": [
        "# **Neurons**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1A_HpoQy39x",
        "outputId": "0beb4661-e282-4c1e-adfc-bc7c86a71033"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "# Hyperparameters range\n",
        "neurons = [8, 16,32, 64, 128]\n",
        "\n",
        "# Randomly select hyperparameters\n",
        "\n",
        "def create_model(neurons):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False))\n",
        "    model.add(Bidirectional(GRU(neurons)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.01)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Tentukan jumlah percobaan yang ingin Anda lakukan\n",
        "num_trials = 1\n",
        "\n",
        "# Tentukan nama file untuk menyimpan model terbaik\n",
        "model_checkpoint = ModelCheckpoint('best_model_1.h5', save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
        "\n",
        "# Gunakan EarlyStopping untuk menghentikan pelatihan saat tidak ada peningkatan dalam metrik yang dipantau\n",
        "early_stopping = EarlyStopping(patience=5 ,monitor='val_accuracy', mode='max', verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "callbacks_list = [early_stopping, model_checkpoint]\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "\n",
        "for n in neurons:  # Loop melalui setiap dropout rate\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "\n",
        "    for _ in range(num_trials):\n",
        "        # Di sini Anda perlu membuat model baru untuk setiap iterasi\n",
        "        model = create_model(n)\n",
        "\n",
        "        # Pelatihan model\n",
        "        HistoryBiGRU = model.fit(X_train, Y_train, batch_size=64, epochs=100, validation_data=(X_test, Y_test), callbacks=callbacks_list)\n",
        "\n",
        "        val_accuracy = HistoryBiGRU.history['val_accuracy'][-1]\n",
        "\n",
        "        print(f\"Trial {_ + 1} Hyperparameters:\")\n",
        "        print(f\"Neurons: {n}\")\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t24lnX5_vzJf"
      },
      "source": [
        "# **Dropout 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tasOImHv4cp",
        "outputId": "155712a3-5b9a-45f7-c3b8-4813e08a8481"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "# Hyperparameters range\n",
        "dropout_rate = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "\n",
        "# Randomly select hyperparameters\n",
        "\n",
        "def create_model(dropout):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False))\n",
        "    model.add(Bidirectional(GRU(32)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.01)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Tentukan jumlah percobaan yang ingin Anda lakukan\n",
        "num_trials = 1\n",
        "\n",
        "# Tentukan nama file untuk menyimpan model terbaik\n",
        "model_checkpoint = ModelCheckpoint('best_model_1.h5', save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
        "\n",
        "# Gunakan EarlyStopping untuk menghentikan pelatihan saat tidak ada peningkatan dalam metrik yang dipantau\n",
        "early_stopping = EarlyStopping(patience=2, monitor='val_accuracy', mode='max', verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "callbacks_list = [early_stopping, model_checkpoint]\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "\n",
        "for do in dropout_rate:  # Loop melalui setiap dropout rate\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "\n",
        "    for _ in range(num_trials):\n",
        "        # Di sini Anda perlu membuat model baru untuk setiap iterasi\n",
        "        model = create_model(do)\n",
        "\n",
        "        # Pelatihan model\n",
        "        HistoryBiGRU = model.fit(X_train, Y_train, batch_size=64, epochs=10, validation_data=(X_test, Y_test), callbacks=callbacks_list)\n",
        "\n",
        "        val_accuracy = HistoryBiGRU.history['val_accuracy'][-1]\n",
        "\n",
        "        print(f\"Trial {_ + 1} Hyperparameters:\")\n",
        "        print(f\"Dropout 1: {do}\")\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1gVw28-26Pq"
      },
      "source": [
        "# **Dense Dim 1**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_ct-27r28GB",
        "outputId": "241a953a-f7ad-4f7f-dbb0-d9cb66997a39"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "# Hyperparameters range\n",
        "dense_dim = [8,16,32,64,128,512]\n",
        "\n",
        "# Randomly select hyperparameters\n",
        "\n",
        "def create_model(dense_1):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False))\n",
        "    model.add(Bidirectional(GRU(32)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(dense_1, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.01)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Tentukan jumlah percobaan yang ingin Anda lakukan\n",
        "num_trials = 1\n",
        "\n",
        "# Tentukan nama file untuk menyimpan model terbaik\n",
        "model_checkpoint = ModelCheckpoint('best_model_1.h5', save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
        "\n",
        "# Gunakan EarlyStopping untuk menghentikan pelatihan saat tidak ada peningkatan dalam metrik yang dipantau\n",
        "early_stopping = EarlyStopping(patience=5, monitor='val_accuracy', mode='max', verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "callbacks_list = [early_stopping, model_checkpoint]\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "\n",
        "for den_1 in dense_dim:  # Loop melalui setiap dropout rate\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "\n",
        "    for _ in range(num_trials):\n",
        "        # Di sini Anda perlu membuat model baru untuk setiap iterasi\n",
        "        model = create_model(n)\n",
        "\n",
        "        # Pelatihan model\n",
        "        HistoryBiGRU = model.fit(X_train, Y_train, batch_size=64, epochs=100, validation_data=(X_test, Y_test), callbacks=callbacks_list)\n",
        "\n",
        "        val_accuracy = HistoryBiGRU.history['val_accuracy'][-1]\n",
        "\n",
        "        print(f\"Trial {_ + 1} Hyperparameters:\")\n",
        "        print(f\"dense_Dim_1: {den_1}\")\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ioh7F65uBGdz"
      },
      "source": [
        "# **Dropoout_2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwNyMDgdBKao",
        "outputId": "b2259a89-e7f3-4436-f7b9-de6c049fe618"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "# Hyperparameters range\n",
        "dropout_rate = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "\n",
        "# Randomly select hyperparameters\n",
        "\n",
        "def create_model(dropout_2):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False))\n",
        "    model.add(Bidirectional(GRU(32)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dropout(dropout_2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.01)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Tentukan jumlah percobaan yang ingin Anda lakukan\n",
        "num_trials = 1\n",
        "\n",
        "# Tentukan nama file untuk menyimpan model terbaik\n",
        "model_checkpoint = ModelCheckpoint('best_model_1.h5', save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
        "\n",
        "# Gunakan EarlyStopping untuk menghentikan pelatihan saat tidak ada peningkatan dalam metrik yang dipantau\n",
        "early_stopping = EarlyStopping(patience=5, monitor='val_accuracy', mode='max', verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "callbacks_list = [early_stopping, model_checkpoint]\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "\n",
        "for do_2 in dropout_rate:  # Loop melalui setiap dropout rate\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "\n",
        "    for _ in range(num_trials):\n",
        "        # Di sini Anda perlu membuat model baru untuk setiap iterasi\n",
        "        model = create_model(do)\n",
        "\n",
        "        # Pelatihan model\n",
        "        HistoryBiGRU = model.fit(X_train, Y_train, batch_size=64, epochs=100, validation_data=(X_test, Y_test), callbacks=callbacks_list)\n",
        "\n",
        "        val_accuracy = HistoryBiGRU.history['val_accuracy'][-1]\n",
        "\n",
        "        print(f\"Trial {_ + 1} Hyperparameters:\")\n",
        "        print(f\"Dropout 2: {do_2}\")\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKcBOBOUAdd8"
      },
      "source": [
        "# **Dense Dim 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPRVANaQAhHL",
        "outputId": "65c4f879-afdf-440e-d8da-5ca576f3e788"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "# Hyperparameters range\n",
        "dense_dim = [8,16,32,64,128,512]\n",
        "\n",
        "# Randomly select hyperparameters\n",
        "\n",
        "def create_model(dense_2):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False))\n",
        "    model.add(Bidirectional(GRU(32)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(dense_2, activation='relu'))\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.01)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Tentukan jumlah percobaan yang ingin Anda lakukan\n",
        "num_trials = 1\n",
        "\n",
        "# Tentukan nama file untuk menyimpan model terbaik\n",
        "model_checkpoint = ModelCheckpoint('best_model_1.h5', save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
        "\n",
        "# Gunakan EarlyStopping untuk menghentikan pelatihan saat tidak ada peningkatan dalam metrik yang dipantau\n",
        "early_stopping = EarlyStopping(patience=5, monitor='val_accuracy', mode='max', verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "callbacks_list = [early_stopping, model_checkpoint]\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "\n",
        "for den_2 in dense_dim:  # Loop melalui setiap dropout rate\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "\n",
        "    for _ in range(num_trials):\n",
        "        # Di sini Anda perlu membuat model baru untuk setiap iterasi\n",
        "        model = create_model(n)\n",
        "\n",
        "        # Pelatihan model\n",
        "        HistoryBiGRU = model.fit(X_train, Y_train, batch_size=64, epochs=100, validation_data=(X_test, Y_test), callbacks=callbacks_list)\n",
        "\n",
        "        val_accuracy = HistoryBiGRU.history['val_accuracy'][-1]\n",
        "\n",
        "        print(f\"Trial {_ + 1} Hyperparameters:\")\n",
        "        print(f\"dense_Dim_2: {den_2}\")\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhNvUdlb5zuy"
      },
      "source": [
        "# **batch size**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsO3GhYb51Rf",
        "outputId": "66736273-4cf3-4c0b-d35c-d2cf19e62310"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "# Hyperparameters range\n",
        "batch_size = [8, 16, 32, 64]\n",
        "\n",
        "# Randomly select hyperparameters\n",
        "\n",
        "def create_model(batch_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False))\n",
        "    model.add(Bidirectional(GRU(32)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "    optimizer = Adam(learning_rate=0.01)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Tentukan jumlah percobaan yang ingin Anda lakukan\n",
        "num_trials = 1\n",
        "\n",
        "# Tentukan nama file untuk menyimpan model terbaik\n",
        "model_checkpoint = ModelCheckpoint('best_model_1.h5', save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
        "\n",
        "# Gunakan EarlyStopping untuk menghentikan pelatihan saat tidak ada peningkatan dalam metrik yang dipantau\n",
        "early_stopping = EarlyStopping(patience=5, monitor='val_accuracy', mode='max', verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "callbacks_list = [early_stopping, model_checkpoint]\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "\n",
        "for bs in batch_size:  # Loop melalui setiap dropout rate\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "\n",
        "    for _ in range(num_trials):\n",
        "        # Di sini Anda perlu membuat model baru untuk setiap iterasi\n",
        "        model = create_model(bs)\n",
        "\n",
        "        # Pelatihan model\n",
        "        HistoryBiGRU = model.fit(X_train, Y_train, batch_size=bs, epochs=100, validation_data=(X_test, Y_test), callbacks=callbacks_list)\n",
        "\n",
        "        val_accuracy = HistoryBiGRU.history['val_accuracy'][-1]\n",
        "\n",
        "        print(f\"Trial {_ + 1} Hyperparameters:\")\n",
        "        print(f\"batch_size: {bs}\")\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7Su_CqByW6R"
      },
      "source": [
        "**Model dengan Parameter Terpilih**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIQ4FQbkGtep",
        "outputId": "37135c07-edb2-4881-cd7d-004e445f995e"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dropout, BatchNormalization, Dense, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Definisi model di luar fungsi\n",
        "model_2 = Sequential()\n",
        "model_2.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False))\n",
        "model_2.add(Bidirectional(GRU(32)))\n",
        "model_2.add(Dropout(0.5))\n",
        "model_2.add(Dense(16, activation='relu'))\n",
        "model_2.add(Dropout(0.4))\n",
        "model_2.add(Dense(32, activation='relu'))\n",
        "model_2.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "def create_model_2():\n",
        "    return model_2\n",
        "\n",
        "# Tentukan nama file untuk menyimpan model_2 terbaik\n",
        "model_2_checkpoint = ModelCheckpoint('best_model_6.h5', save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
        "\n",
        "# Gunakan EarlyStopping untuk menghentikan pelatihan saat tidak ada peningkatan dalam metrik yang dipantau\n",
        "early_stopping = EarlyStopping(patience=10, monitor='val_accuracy', mode='max', verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Callbacks list yang berisi kedua callback\n",
        "callbacks_list = [early_stopping, model_2_checkpoint]\n",
        "\n",
        "# Pemanggilan fungsi create_model_2() untuk mendapatkan model_2\n",
        "model_2 = create_model_2()\n",
        "\n",
        "# Pastikan Anda telah mendefinisikan X_train, Y_train, X_test, dan Y_test sebelumnya\n",
        "\n",
        "HistoryBiGRU = model_2.fit(X_train, Y_train, batch_size=64, epochs=50, validation_data=(X_test, Y_test), callbacks=callbacks_list)\n",
        "\n",
        "val_accuracy = HistoryBiGRU.history['val_accuracy'][-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CFEMmw7vGzy"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.models import save_model\n",
        "\n",
        "# # Simpan model ke Google Drive\n",
        "# model_path = '/content/drive/MyDrive/Skripsi/model_skripsi/best_model6/'\n",
        "# save_model(best_model, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaPHT9Fe1HSd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/best_model_6.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "zmUt8ExwJKzY",
        "outputId": "99fbb264-f611-4aeb-e40b-741c23f43a39"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot akurasi pelatihan dan validasi\n",
        "plt.plot(HistoryBiGRU.history['accuracy'])\n",
        "plt.plot(HistoryBiGRU.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot loss pelatihan dan validasi\n",
        "plt.plot(HistoryBiGRU.history['loss'])\n",
        "plt.plot(HistoryBiGRU.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTXfa7Unrwn9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VbIyLGqn9uF",
        "outputId": "856ed03e-6ba9-4e36-8537-4907c1db51f0"
      },
      "outputs": [],
      "source": [
        "# prediksi classes untuk data test\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = y_pred.argmax(axis=1)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)\n",
        "print(classification_report(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, digits=4, output_dict=False))\n",
        "report = classification_report(y_true, y_pred,\n",
        "digits=4, output_dict=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "NK9ljlxtuLcX",
        "outputId": "b807b9e2-e688-436c-cfdc-7abd3588868e"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(Y_test, axis=1)\n",
        "# confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LKDvxfVJp8H"
      },
      "source": [
        "# **KLASIFIKASI DATA BARU**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z7Ck0K8yiZU"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "id": "Xe7_M2z7pbaI",
        "outputId": "83e0ee5b-9596-4333-e2ab-0ac4b090c417"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "file_prediksi=pd.read_excel('/content/data_baru_klasifikasi.xlsx')\n",
        "file_prediksi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gnmXzkib0jF",
        "outputId": "12034f08-20ae-4113-ca50-416e14267120"
      },
      "outputs": [],
      "source": [
        "#Case Folding\n",
        "file_prediksi['textDisplay'] = file_prediksi['textDisplay'].str.lower()\n",
        "print('Case Folding Result : \\n')\n",
        "print(file_prediksi['textDisplay'].head(5))\n",
        "print('\\n\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fopL7wzqcT4H",
        "outputId": "7bacbf41-646c-406e-9ca9-a2a9a3d0a620"
      },
      "outputs": [],
      "source": [
        "import string, re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ8ZFKpLcYgN",
        "outputId": "dc0ce4fc-1c7b-48c2-e950-de3ed882fa96"
      },
      "outputs": [],
      "source": [
        "#Tokenizing\n",
        "def remove_comments_special(text):\n",
        "    #menghapus karakter khusus dalam teks, seperti tab, newline, dan backslash\n",
        "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\" \").replace('.',\" \").replace(',',\" \")\n",
        "    # remove non ASCII (emoticon, chinese word, .etc)\n",
        "    text = text.encode('ascii', 'replace').decode('ascii')\n",
        "    # remove mention, link, hashtag\n",
        "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
        "    # remove incomplete URL\n",
        "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
        "file_prediksi['textDisplay'] = file_prediksi['textDisplay'].apply(remove_comments_special)\n",
        "\n",
        "#menghapus angka dalam teks dengan menggunakan regular expression\n",
        "def remove_number(text):\n",
        "    return  re.sub(r\"\\d+\", \" \", text)\n",
        "file_prediksi['textDisplay'] = file_prediksi['textDisplay'].apply(remove_number)\n",
        "#menghapus emotikon dari teks menggunakan regular expression\n",
        "def remove_emoticons(text):\n",
        "    emoticon_pattern = re.compile(\"[\"\n",
        "                                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                                 u\"\\U0001F300-\\U0001F5FF\"  # simbol & piktogram\n",
        "                                 u\"\\U0001F680-\\U0001F6FF\"  # transportasi & simbol peralatan\n",
        "                                 u\"\\U0001F1E0-\\U0001F1FF\"  # bendera negara\n",
        "                                 u\"\\U00002702-\\U000027B0\"  # simbol lainnya\n",
        "                                 u\"\\U000024C2-\\U0001F251\"\n",
        "                                 \"]+\", flags=re.UNICODE)\n",
        "    return emoticon_pattern.sub(r'', text)\n",
        "file_prediksi['textDisplay'] = file_prediksi['textDisplay'].apply(remove_emoticons)\n",
        "#file_prediksi['review'] = file_prediksi['review'].apply(remove_number)\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    return text.translate(translator).replace(\",\", \" \")\n",
        "file_prediksi['textDisplay'] = file_prediksi['textDisplay'].apply(remove_punctuation)\n",
        "# menghapus spasi yang ada di awal dan akhir teks menggunakan str.strip()\n",
        "def remove_whitespace_LT(text):\n",
        "    return text.strip()\n",
        "file_prediksi['textDisplay'] = file_prediksi['textDisplay'].apply(remove_whitespace_LT)\n",
        "#menghapus spasi berlebih dalam teks dengan menggantinya dengan satu spasi menggunakan regular expression\n",
        "def remove_extra_spaces(text):\n",
        "    return re.sub(r'\\s+', ' ', text)\n",
        "file_prediksi['textDisplay'] = file_prediksi['textDisplay'].apply(remove_extra_spaces)\n",
        "#mengganti multiple whitespace (spasi berturut-turut) dengan satu spasi menggunakan regular expression\n",
        "def remove_whitespace_multiple(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "file_prediksi['textDisplay'] = file_prediksi['textDisplay'].apply(remove_whitespace_multiple)\n",
        "#menghapus huruf yang berulang dalam teks\n",
        "def remove_repeated_letters(text):\n",
        "    return re.sub(r\"(.)\\1+\", r\"\\1\", text)\n",
        "file_prediksi['textDisplay'] = file_prediksi['textDisplay'].apply(remove_repeated_letters)\n",
        "\n",
        "def filter_length(text):\n",
        "    # Menghapus kata-kata yang kurang dari 4 karakter atau lebih dari 20 karakter\n",
        "    text_filtered = ' '.join(word for word in re.split(r'\\s', text) if 3 < len(word) < 26)\n",
        "    return text_filtered\n",
        "file_prediksi['textDisplay'] = file_prediksi['textDisplay'].apply(filter_length)\n",
        "\n",
        "#mengubah textDisplay menjadi token\n",
        "def word_tokenize_wrapper(text):\n",
        "    return word_tokenize(text)\n",
        "file_prediksi['comments_tokens'] = file_prediksi['textDisplay'].apply(word_tokenize_wrapper)\n",
        "print('Tokenizing Result : \\n')\n",
        "print(file_prediksi['comments_tokens'].head())\n",
        "print('\\n\\n\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3113
        },
        "id": "aDSlFWMBq0zZ",
        "outputId": "11c0cf2b-8a72-4261-f8fc-480c7d8584e5"
      },
      "outputs": [],
      "source": [
        "file_prediksi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aPVTTTTctZH"
      },
      "outputs": [],
      "source": [
        "#Spelling Normalization\n",
        "normalizad_word = pd.read_csv(\"https://raw.githubusercontent.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset/master/kamus_singkatan.csv\", sep=\";\", header=None)\n",
        "normalizad_word_dict = {}\n",
        "\n",
        "for index, row in normalizad_word.iterrows():\n",
        "    if row[0] not in normalizad_word_dict:\n",
        "        normalizad_word_dict[row[0]] = row[1]\n",
        "\n",
        "def normalized_term(document):\n",
        "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
        "file_prediksi['comments_normalized'] = file_prediksi['comments_tokens'].apply(normalized_term)\n",
        "\n",
        "normalizad_word2 = pd.read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRQS3tlUL5EcxYqbbYzFLHmHaqm2npjY-DLyz0dzwMIcUVhfoVWKuhR52P9YCqbAyY9zCgT66JVutWA/pub?output=csv\",header=None)\n",
        "normalizad_word_dict2 = {}\n",
        "\n",
        "for index, row in normalizad_word2.iterrows():\n",
        "    if row[0] not in normalizad_word_dict2:\n",
        "        normalizad_word_dict2[row[0]] = row[1]\n",
        "def normalized_term2(document):\n",
        "    return [normalizad_word_dict2[term] if term in normalizad_word_dict2 else term for term in document]\n",
        "file_prediksi['comments_normalized'] = file_prediksi['comments_normalized'].apply(normalized_term2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWsWPQD_lSNC",
        "outputId": "8a4c0c99-31e2-4070-942c-9a5dc05247f5"
      },
      "outputs": [],
      "source": [
        "#menyatukan kembali tokenizer menjadi kalimat\n",
        "def detokenize(token_list):\n",
        "    # Menggabungkan token dengan spasi sebagai pemisah\n",
        "    text = ' '.join(token_list)\n",
        "    return text\n",
        "\n",
        "file_prediksi['comments_join'] = file_prediksi['comments_normalized'].apply(detokenize)\n",
        "\n",
        "print(file_prediksi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43MMvavcldLW",
        "outputId": "fb883441-c9c4-4141-f8ca-66f3c76e835a"
      },
      "outputs": [],
      "source": [
        "# menghapus spasi yang ada di awal dan akhir teks menggunakan str.strip()\n",
        "def remove_whitespace_LT(text):\n",
        "    return text.strip()\n",
        "file_prediksi['comments_join'] = file_prediksi['comments_join'].apply(remove_whitespace_LT)\n",
        "#menghapus spasi berlebih dalam teks dengan menggantinya dengan satu spasi menggunakan regular expression\n",
        "def remove_extra_spaces(text):\n",
        "    return re.sub(r'\\s+', ' ', text)\n",
        "file_prediksi['comments_join'] = file_prediksi['comments_join'].apply(remove_extra_spaces)\n",
        "#mengganti multiple whitespace (spasi berturut-turut) dengan satu spasi menggunakan regular expression\n",
        "def remove_whitespace_multiple(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "file_prediksi['comments_join'] = file_prediksi['comments_join'].apply(remove_whitespace_multiple)\n",
        "\n",
        "#mengubah kalimat menjadi token\n",
        "def word_tokenize_wrapper(text):\n",
        "    return word_tokenize(text)\n",
        "file_prediksi['comments_tokens_final'] = file_prediksi['comments_join'].apply(word_tokenize_wrapper)\n",
        "print('Tokenizing Result : \\n')\n",
        "print(file_prediksi['comments_tokens_final'].head())\n",
        "print('\\n\\n\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3200
        },
        "id": "kDfEoSDrdCep",
        "outputId": "1c24c772-dafd-4aa9-af30-e6968b314605"
      },
      "outputs": [],
      "source": [
        "#Stopword Removal\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Skripsi/data_skripsi/stop word indo (3).txt'  # Ganti dengan jalur file yang sesuai\n",
        "\n",
        "# Baca isi file teks ke dalam list\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Mengubah setiap baris menjadi string dengan kutipan ganda\n",
        "formatted_lines = ['\"' + line.strip() + '\"' for line in lines]\n",
        "\n",
        "# Menggabungkan baris-baris menjadi satu string dengan koma sebagai pemisah\n",
        "formatted_text = \", \".join(formatted_lines)\n",
        "\n",
        "print(formatted_text)\n",
        "\n",
        "# List kata stopwords\n",
        "stopwords_list = formatted_text\n",
        "def stopwords_removal(words):\n",
        "    return [word for word in words if word not in stopwords_list]\n",
        "\n",
        "\n",
        "file_prediksi['comments_tokens_final'] = file_prediksi['comments_tokens_final'].apply(stopwords_removal)\n",
        "file_prediksi.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XF7j7najv4MD"
      },
      "outputs": [],
      "source": [
        "# file_prediksi.to_excel('/content/prediksi.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7VxUVFzwez0",
        "outputId": "6b475ebf-fcfb-4eca-e8e7-4cf31e6691b6"
      },
      "outputs": [],
      "source": [
        "#Stemming\n",
        "# import Sastrawi package\n",
        "! pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "# create stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "#stemmed\n",
        "def stemmed_wrapper(term):\n",
        "    return stemmer.stem(term)\n",
        "term_dict = {}\n",
        "for document in file_prediksi['comments_tokens_final']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = ' '\n",
        "print(len(term_dict))\n",
        "print(\"------------------------\")\n",
        "for term in term_dict:\n",
        "    term_dict[term] = stemmed_wrapper(term)\n",
        "    print(term,\":\" ,term_dict[term])\n",
        "print(term_dict)\n",
        "print(\"------------------------\")\n",
        "# apply stemmed term to file_prediksiframe\n",
        "def get_stemmed_term(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "file_prediksi['comments_tokens_stemmed'] = file_prediksi['comments_tokens_final'].apply(get_stemmed_term)\n",
        "\n",
        "# file_prediksi[\"Ulasan_clean\"] = [' '.join(map(str, l)) for l in file_prediksi['comments_tokens_stemmed']]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "uHwcBwO_0DXm",
        "outputId": "2f5505ef-f370-48b4-f01d-8a063b75b48a"
      },
      "outputs": [],
      "source": [
        "#Stopword Removal\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Skripsi/data_skripsi/stop word indo (3).txt'  # Ganti dengan jalur file yang sesuai\n",
        "\n",
        "# Baca isi file teks ke dalam list\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Mengubah setiap baris menjadi string dengan kutipan ganda\n",
        "formatted_lines = ['\"' + line.strip() + '\"' for line in lines]\n",
        "\n",
        "# Menggabungkan baris-baris menjadi satu string dengan koma sebagai pemisah\n",
        "formatted_text = \", \".join(formatted_lines)\n",
        "\n",
        "print(formatted_text)\n",
        "\n",
        "# List kata stopwords\n",
        "stopwords_list = formatted_text\n",
        "def stopwords_removal(words):\n",
        "    return [word for word in words if word not in stopwords_list]\n",
        "\n",
        "\n",
        "file_prediksi['comments_tokens_stemmed'] = file_prediksi['comments_tokens_stemmed'].apply(stopwords_removal)\n",
        "file_prediksi.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEogse141Pvu"
      },
      "outputs": [],
      "source": [
        "file_prediksi[\"Ulasan_clean\"] = [' '.join(map(str, l)) for l in file_prediksi['comments_tokens_stemmed']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Rv7ZnmUYrzJ"
      },
      "outputs": [],
      "source": [
        "def remove_rows_without_words(row):\n",
        "    return len(row['Ulasan_clean'].split()) > 0  # Jika jumlah kata lebih dari 0, baris tetap\n",
        "\n",
        "# Menghapus baris yang tidak memiliki kata dalam kolom Ulasan_Clean\n",
        "file_prediksi = file_prediksi[file_prediksi.apply(remove_rows_without_words, axis=1)]\n",
        "# Hapus baris dengan kolom 'Ulasan_clean' tanpa nilai (empty)\n",
        "file_prediksi = file_prediksi.dropna(subset=['Ulasan_clean'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "OTNy1PUoUeg6",
        "outputId": "46f91818-80b8-4f3f-bd90-5d9a5f802bbb"
      },
      "outputs": [],
      "source": [
        "file_prediksi.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6WAqm3w5-sg",
        "outputId": "d98a24c5-9d0a-4812-953b-582c8af96bbf"
      },
      "outputs": [],
      "source": [
        "# Count the number of words in each list of 'comments_tokens_sw'\n",
        "file_prediksi['word_count'] = file_prediksi['comments_tokens_stemmed'].apply(lambda x: len(x))\n",
        "\n",
        "# Filter rows where the word count is greater than or equal to 5\n",
        "file_prediksi = file_prediksi[file_prediksi['word_count'] >= 4]\n",
        "\n",
        "# Drop the 'word_count' column if you don't need it anymore\n",
        "file_prediksi = file_prediksi.drop(columns=['word_count'])\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(file_prediksi.head())\n",
        "file_prediksi.to_excel(\"Data_Skripsi_Bersih_prediksi_databaru.xlsx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73gkJvgLhDpw",
        "outputId": "bdeff303-1330-49b7-d5f0-85a53f791816"
      },
      "outputs": [],
      "source": [
        "print(file_prediksi.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9aNC8KHypFr"
      },
      "source": [
        "**Klasifikasi Data Dengan Model Bi-GRU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dXRGIrD898u",
        "outputId": "da185eca-a09c-4b9a-deec-8cd7444b334f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/content/Data_Skripsi_Bersih_prediksi_databaru.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# # Menghapus baris dengan nilai NaN pada kolom 'Ulasan_Clean'\n",
        "# data = data.dropna(subset=['Ulasan_clean'])\n",
        "\n",
        "# Preprocess and predict for each sentence in the Excel file\n",
        "predicted_sentiments = []\n",
        "\n",
        "for sentence in data['Ulasan_clean']:\n",
        "    # Pre-processing steps like tokenization, normalization, removing stopwords, stemming, etc.\n",
        "    new_sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    new_sequence = pad_sequences(new_sequence, maxlen=100)\n",
        "\n",
        "    # Perform prediction using the model\n",
        "    new_prediction = model.predict(new_sequence)\n",
        "    predicted_class = np.argmax(new_prediction)\n",
        "\n",
        "    if predicted_class == 0:\n",
        "        predicted_sentiments.append(\"Sentimen Negatif\")\n",
        "    else:\n",
        "        predicted_sentiments.append(\"Sentimen Positif\")\n",
        "\n",
        "# Add the predicted sentiments to the dataFrame\n",
        "data['Prediksi Sentimen'] = predicted_sentiments\n",
        "\n",
        "# Save the dataFrame with predictions back to the Excel file\n",
        "data.to_excel(file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGhyx8TDLE70"
      },
      "source": [
        "# **Topic Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZLWlfB-LNia",
        "outputId": "625294d0-b195-4e9a-be2b-f1a8bb445d66"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B_fMDzcLVuF",
        "outputId": "df5413f1-2a12-4b5c-9ffe-e9e3c5ad31f4"
      },
      "outputs": [],
      "source": [
        "pip install pyldavis==3.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhlZjnF2LZ9t",
        "outputId": "ca5b3d98-6075-41fa-a33c-e33d95a75b32"
      },
      "outputs": [],
      "source": [
        "pip install swifter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqi_wGhtLfz_"
      },
      "source": [
        "**Preparing Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "OU7Kc6fCMvwk",
        "outputId": "e50ebd88-9add-4efb-f90b-4fbf06e9f069"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "data=pd.read_excel('/content/Data_Skripsi_Bersih_prediksi_databaru.xlsx')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeZNk8MY5G9e"
      },
      "outputs": [],
      "source": [
        "# Filter rows where the word count is greater than or equal to 5\n",
        "data['word_count'] = data['Ulasan_clean'].apply(lambda x: len(x.split()))\n",
        "data = data[data['word_count'] > 4]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb9hscXmOLOl"
      },
      "source": [
        "# **LDA MODEL USING GENSIM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9y4G7usNepU",
        "outputId": "0cbac2d6-4a8e-4954-fd87-3fb636b08fc5"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Filter rows where the sentiment is negative\n",
        "negative_sentiment_data = data[data['Prediksi Sentimen'] == 'Sentimen Negatif']\n",
        "\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(negative_sentiment_data.head())\n",
        "doc_clean = negative_sentiment_data[\"Ulasan_clean\"].apply(lambda x: x.split())\n",
        "\n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "print(dictionary)\n",
        "\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do5QsFaeHs5n",
        "outputId": "a4a84541-a87d-42f3-b6d0-493193e061a4"
      },
      "outputs": [],
      "source": [
        "negative_sentiment_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "q9LmZUelF4DH",
        "outputId": "05ce4e5e-4994-45ae-9488-5316311feae7"
      },
      "outputs": [],
      "source": [
        "###WordCloud\n",
        "#Import Library untuk WordCloud\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "text = \" \".join(title for title in negative_sentiment_data[\"Ulasan_clean\"])\n",
        "word_cloud = WordCloud(collocations = False, background_color = 'white',\n",
        "                        width = 2048, height = 1080).generate(text)\n",
        "#Menyimpan Gambar WordCloud\n",
        "word_cloud.to_file('wordcloud.png')\n",
        "#Menampilkan Hasil WordCloud\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3bV73EaIjDV"
      },
      "source": [
        "# **HYPERPARAMETER TOPIK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Cb2hSMsN_be"
      },
      "outputs": [],
      "source": [
        "from gensim.models import CoherenceModel, LdaModel\n",
        "\n",
        "\n",
        "# Assuming you have already defined doc_clean, doc_term_matrix, and dictionary\n",
        "\n",
        "corpus = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "\n",
        "def evaluate_model(model, corpus, texts, num_topics):\n",
        "    # Compute coherence score\n",
        "    coherence_model = CoherenceModel(model=model, texts=doc_clean, corpus=corpus, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "    return coherence_score\n",
        "\n",
        "# Running and Training LDA model on the document term matrix.\n",
        "total_topics = [2, 3, 4, 5, 6, 7]  # Or other values as needed\n",
        "\n",
        "highest_coherence_score = 0\n",
        "best_model = None\n",
        "\n",
        "for num_topics in total_topics:\n",
        "    # Initialize model with a specific number of topics\n",
        "    model = LdaModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "\n",
        "\n",
        "    # Evaluate model\n",
        "    coherence_score = evaluate_model(model, corpus, doc_clean, num_topics)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Num Topics: {num_topics}, Coherence Score: {coherence_score}\")\n",
        "\n",
        "    # Check if the current model has a higher coherence score\n",
        "    if coherence_score > highest_coherence_score:\n",
        "        highest_coherence_score = coherence_score\n",
        "        best_model = model\n",
        "\n",
        "# Save the best model\n",
        "best_model.save(\"best_lda_model\")\n",
        "\n",
        "# Print information about the best model\n",
        "print(f\"Best Model - Num Topics: {best_model.num_topics}, Coherence Score: {highest_coherence_score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJy4oRwWfXvr"
      },
      "outputs": [],
      "source": [
        "from gensim.models import LdaModel\n",
        "best_model = LdaModel.load(\"/content/best_lda_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbpO7nY5NoZc",
        "outputId": "35060f5f-8839-40a3-dbde-446639ef4c5d"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, corpus, texts, num_topics):\n",
        "    # Compute coherence score\n",
        "    coherence_model = CoherenceModel(model=best_model, texts=texts, corpus=corpus, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "    # Print coherence score\n",
        "    print(f\"Coherence Score for {num_topics} topics: {coherence_score}\")\n",
        "\n",
        "    # Print beta values for each topic\n",
        "    for topic_id in range(num_topics):\n",
        "        topic_terms = best_model.get_topic_terms(topicid=topic_id, topn=10)  # Adjust topn as needed\n",
        "        print(f\"Topik {topic_id}:\")\n",
        "        for term_id, beta in topic_terms:\n",
        "            print(f\"- Kata: {dictionary[term_id]}, Nilai Beta: {beta:.2f}\")  # Format untuk 2 angka di belakang koma\n",
        "\n",
        "    return coherence_score\n",
        "\n",
        "# Print information about the best model\n",
        "\n",
        "print(\"Beta values for best model:\")\n",
        "evaluate_model(best_model, corpus, doc_clean, best_model.num_topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy7MhXFiIoen"
      },
      "source": [
        "# **HYPERPARAMETER ALPHA DAN BETA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZb1wJoIz-El",
        "outputId": "d59da4b3-3edb-47d9-e065-893514de90f0"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Define a function to evaluate model with specific alpha and beta\n",
        "def evaluate_model_with_hyperparams(corpus, texts, num_topics, alpha, beta):\n",
        "    model = LdaModel(corpus=corpus, num_topics=4, id2word=dictionary, passes=10, alpha=alpha, eta=beta)\n",
        "    coherence_model = CoherenceModel(model=model, texts=texts, corpus=corpus, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    return coherence_score, model\n",
        "\n",
        "# Running and Training LDA model on the document term matrix.\n",
        "total_alpha_values = [0.01,0.1,1,'symmetric', 'asymmetric']  # Alpha values to try\n",
        "total_beta_values = [0.01,0.1,1,'symmetric']   # Beta values to try\n",
        "best_coherence_score = 0\n",
        "best_alpha = None\n",
        "best_beta = None\n",
        "best_model = None\n",
        "\n",
        "for alpha in total_alpha_values:\n",
        "    for beta in total_beta_values:\n",
        "        # Evaluate model with specific alpha and beta\n",
        "        coherence_score, model = evaluate_model_with_hyperparams(corpus, doc_clean, num_topics=4, alpha=alpha, beta=beta)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Alpha: {alpha}, Beta: {beta}, Coherence Score: {coherence_score}\")\n",
        "\n",
        "        # Check if the current model has a higher coherence score\n",
        "        if coherence_score > best_coherence_score:\n",
        "            best_coherence_score = coherence_score\n",
        "            best_alpha = alpha\n",
        "            best_beta = beta\n",
        "            best_model = model\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best Alpha: {best_alpha}, Best Beta: {best_beta}, Best Coherence Score: {best_coherence_score}\")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save(\"best_lda_model_hyperparameter\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEwINqY9597k",
        "outputId": "beadb73d-dac1-4aa7-c743-8430587848d4"
      },
      "outputs": [],
      "source": [
        "from gensim.models import LdaModel\n",
        "best_model_hyper = LdaModel.load(\"/content/best_lda_model_hyperparameter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB_o6Sml6iks",
        "outputId": "50db05e0-c409-434d-9e34-def8c3d13a4f"
      },
      "outputs": [],
      "source": [
        "# Check alpha\n",
        "alpha = best_model.alpha\n",
        "\n",
        "# Check beta\n",
        "beta = best_model.eta\n",
        "\n",
        "print(\"Alpha:\", alpha)\n",
        "print(\"Beta:\", beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAKCq5FJ7dOu",
        "outputId": "605fb92f-b2dd-4c9d-d8fc-2067ed3ac336"
      },
      "outputs": [],
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Menghitung skor koherensi menggunakan CoherenceModel\n",
        "coherence_model = CoherenceModel(model=best_model_hyper, texts=doc_clean, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "# Print skor koherensi\n",
        "print(\"Coherence Score:\", coherence_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPU_Zipd2R3Q",
        "outputId": "3299d319-be05-4a7c-feb5-ac619830485c"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, corpus, texts, num_topics):\n",
        "    # Compute coherence score\n",
        "    coherence_model = CoherenceModel(model=best_model, texts=texts, corpus=corpus, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "    # Print coherence score\n",
        "    print(f\"Coherence Score for {num_topics} topics: {coherence_score}\")\n",
        "\n",
        "    # Print beta values for each topic\n",
        "    for topic_id in range(num_topics):\n",
        "        topic_terms = best_model.get_topic_terms(topicid=topic_id, topn=10)  # Adjust topn as needed\n",
        "        print(f\"Topik {topic_id}:\")\n",
        "        for term_id, beta in topic_terms:\n",
        "            print(f\"- Kata: {dictionary[term_id]}, Nilai Beta: {beta:.2f}\")  # Format untuk 2 angka di belakang koma\n",
        "\n",
        "    return coherence_score\n",
        "\n",
        "# Print information about the best model\n",
        "\n",
        "print(\"Beta values for best model:\")\n",
        "evaluate_model(best_model, corpus, doc_clean, best_model.num_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUQvRUWnbVAk"
      },
      "outputs": [],
      "source": [
        "doc_clean.to_excel('nama_file1.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9vihkfpQc7X",
        "outputId": "abcdad22-db96-4dd2-8c09-2ad1a6ec3e59"
      },
      "outputs": [],
      "source": [
        "# Word Count of Topic Keywords\n",
        "\n",
        "from collections import Counter\n",
        "topics = best_model.show_topics(formatted=False)\n",
        "data_flat = [w for w_list in doc_clean for w in w_list]\n",
        "counter = Counter(data_flat)\n",
        "\n",
        "out = []\n",
        "for i, topic in topics:\n",
        "    for word, weight in topic:\n",
        "        out.append([word, i , weight, counter[word]])\n",
        "\n",
        "df_imp_wcount = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])\n",
        "print(df_imp_wcount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMVxS0aqQe9G"
      },
      "outputs": [],
      "source": [
        "#Dominant topic and its percentage contribution in each topic\n",
        "def format_topics_sentences(ldamodel=None, corpus=doc_term_matrix, texts=doc_clean):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlGi2-91Qh5e",
        "outputId": "32528736-aec4-410f-da7c-eae448790413"
      },
      "outputs": [],
      "source": [
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=best_model, corpus=doc_term_matrix, texts=doc_clean)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "print(df_dominant_topic.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJCC2LjWQjvy"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis.gensim\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV3Mos5yQo9x",
        "outputId": "0fcc8664-28ab-4864-cd14-98e37c38074e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "LDAvis_data_filepath = os.path.join('ldavis_prepared_'+str(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbtMaiCFQsKv",
        "outputId": "d21e964d-93c6-4648-f14e-00e70a1d4260"
      },
      "outputs": [],
      "source": [
        "corpus = [dictionary.doc2bow(text) for text in doc_clean]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQWg9Nl0Qtyr",
        "outputId": "f12fc2a6-6631-44f8-c1ed-1acafed4f40b"
      },
      "outputs": [],
      "source": [
        "if 1 == 1:\n",
        "    LDAvis_prepared = pyLDAvis.gensim.prepare(best_model, corpus, dictionary)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiOxq3eyQwrf",
        "outputId": "f8092875-8cd3-4fbb-b8b3-a222f6813219"
      },
      "outputs": [],
      "source": [
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "hTspVOXiQzcm",
        "outputId": "64d66262-9818-4a94-f70e-f4b48d5b8460"
      },
      "outputs": [],
      "source": [
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuLf8t5_Q5gy",
        "outputId": "39f7f0c6-a3c7-4d7b-a797-5719acaa95f6"
      },
      "outputs": [],
      "source": [
        "# Mencetak nama-nama kolom\n",
        "print(df_topic_sents_keywords.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2R_VApzlQ70T",
        "outputId": "0bf8e005-6fa4-442f-da6b-71b909729071"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Contoh: Membuat word cloud untuk setiap topik\n",
        "for topic_id in df_imp_wcount['topic_id'].unique():\n",
        "    # Filter DataFrame untuk mendapatkan data terkait topik tertentu\n",
        "    topic_data = df_imp_wcount[df_imp_wcount['topic_id'] == topic_id]\n",
        "\n",
        "    # Menggabungkan kata-kata kunci dari semua kata dalam topik tersebut\n",
        "    topic_keywords = ' '.join(topic_data['word'])\n",
        "\n",
        "    # Membuat word cloud\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(topic_keywords)\n",
        "\n",
        "    # Menampilkan word cloud\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Word Cloud for Topic {topic_id}\")\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.11 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "65a440aeac0c89e2af7569e0aa53b64434c4b69eb6285e2b0d174d9bca190d54"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
